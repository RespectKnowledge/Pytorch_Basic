{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autograd_pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAfsEMqbePgL",
        "outputId": "d2d6c31a-0c62-4edd-c62e-4b81efc5fb59"
      },
      "source": [
        "import torch\r\n",
        "x=torch.tensor([[1.,-1.],[1.,1.]],requires_grad=True)\r\n",
        "out=x.pow(2).sum()\r\n",
        "out.backward()\r\n",
        "print(x)\r\n",
        "print(out)\r\n",
        "print(x.grad)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1., -1.],\n",
            "        [ 1.,  1.]], requires_grad=True)\n",
            "tensor(4., grad_fn=<SumBackward0>)\n",
            "tensor([[ 2., -2.],\n",
            "        [ 2.,  2.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoHnxPDiksS7",
        "outputId": "e56d912d-6086-4037-9c64-5f07d1676ebf"
      },
      "source": [
        "x=torch.randn(3,requires_grad=True)\r\n",
        "y=x+2\r\n",
        "z=y*y*+2\r\n",
        "z=z.mean()\r\n",
        "z.backward()\r\n",
        "print(x.grad)\r\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([4.8938, 3.7899, 4.3716])\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MCyNGnfls4D",
        "outputId": "5221eff4-8ff8-4d3a-e994-47c55740c64c"
      },
      "source": [
        "x=torch.randn(3,requires_grad=True)\r\n",
        "y=x+2\r\n",
        "z=y*y*+2\r\n",
        "v=torch.tensor([1.,1.00,1.1],dtype=torch.float32) # need vector to compute gradient\r\n",
        "z.backward(v)\r\n",
        "print(x.grad)\r\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 5.9261, 11.6729,  5.9292])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "AK3U9TZAlQmb",
        "outputId": "e3ed7d7d-5251-4f1c-949f-f7c9c5529a7b"
      },
      "source": [
        "x=torch.randn(3)\r\n",
        "y=x+2\r\n",
        "z=y*y*+2\r\n",
        "z=z.mean()\r\n",
        "z.backward() # gives error due to deafult requires_grad funtion is false\r\n",
        "print(x.grad)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-4231043e8403>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# gives error due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "lUhb59_aleNJ",
        "outputId": "ea319180-7ffd-4bdd-f348-53d8fac548dd"
      },
      "source": [
        "x=torch.randn(3, requires_grad=False)\r\n",
        "y=x+2\r\n",
        "z=y*y*+2\r\n",
        "z=z.mean()\r\n",
        "z.backward() # gives error due to deafult requires_grad funtion is false\r\n",
        "print(x.grad)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-3aae16d33cdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# gives error due to deafult requires_grad funtion is false\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQwy0CdgliKx",
        "outputId": "006ed348-2a01-45f2-d1f2-f23d74137c7f"
      },
      "source": [
        "x=torch.randn(5,requires_grad=True)\r\n",
        "print(x)\r\n",
        "y=x+2\r\n",
        "print(y)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-0.3096,  0.5154, -0.1457, -0.3522, -0.0773], requires_grad=True)\n",
            "tensor([1.6904, 2.5154, 1.8543, 1.6478, 1.9227], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1suILRHcfNHb",
        "outputId": "72fc2b02-11bd-4ec2-f991-039d54b23e2c"
      },
      "source": [
        "# create tensors\r\n",
        "x=torch.tensor(1., requires_grad=True)\r\n",
        "w=torch.tensor(2.,requires_grad=True)\r\n",
        "b=torch.tensor(3., requires_grad=True)\r\n",
        "y=w*x+b\r\n",
        "y.backward()\r\n",
        "print(x.grad)\r\n",
        "print(w.grad)\r\n",
        "print(b.grad)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.)\n",
            "tensor(1.)\n",
            "tensor(1.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZkL6fMggEML",
        "outputId": "646e46e3-4ae8-4db4-a843-933958fc7318"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "# create torch tensor\r\n",
        "x=torch.randn(10,3)\r\n",
        "y=torch.randn(10,2)\r\n",
        "# build linear model\r\n",
        "linearm=nn.Linear(3,2)\r\n",
        "print(\"w:\",linearm.weight)\r\n",
        "print(\"bias:\",linearm.bias)\r\n",
        "# build loss function and optimizers\r\n",
        "loss_fun=nn.MSELoss()\r\n",
        "optimizer=torch.optim.SGD(linearm.parameters(),lr=0.01)\r\n",
        "# forward \r\n",
        "pred=linearm(x)\r\n",
        "# compute loss\r\n",
        "loss=loss_fun(pred,y)\r\n",
        "# print the loss\r\n",
        "print(\"loss\",loss.item())\r\n",
        "# backward\r\n",
        "loss.backward()\r\n",
        "# print the gradients\r\n",
        "print(\"gradient w:\",linearm.weight.grad)\r\n",
        "print(\"gradient b:\",linearm.bias.grad)\r\n",
        "# update the gradient\r\n",
        "optimizer.step()\r\n",
        "########################## first iteration ##################\r\n",
        "# forward \r\n",
        "pred=linearm(x)\r\n",
        "# compute loss\r\n",
        "loss=loss_fun(pred,y)\r\n",
        "# print the loss\r\n",
        "print(\"loss after one iteration\",loss.item())\r\n",
        "# backward\r\n",
        "loss.backward()\r\n",
        "optimizer.zero_grad()\r\n",
        "# print the gradients\r\n",
        "print(\"gradient w:\",linearm.weight.grad)\r\n",
        "print(\"gradient b:\",linearm.bias.grad)\r\n",
        "# update the gradient\r\n",
        "optimizer.step()\r\n",
        "\r\n",
        "################################### second ieration ################\r\n",
        "# forward \r\n",
        "pred=linearm(x)\r\n",
        "# compute loss\r\n",
        "loss=loss_fun(pred,y)\r\n",
        "# print the loss\r\n",
        "print(\"loss after second iteration\",loss.item())\r\n",
        "optimizer.zero_grad()\r\n",
        "# print the gradients\r\n",
        "print(\"gradient w:\",linearm.weight.grad)\r\n",
        "print(\"gradient b:\",linearm.bias.grad)\r\n",
        "# update the gradient\r\n",
        "optimizer.step()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w: Parameter containing:\n",
            "tensor([[-0.2236,  0.3833, -0.3175],\n",
            "        [-0.2632,  0.0868, -0.0591]], requires_grad=True)\n",
            "bias: Parameter containing:\n",
            "tensor([ 0.5487, -0.0647], requires_grad=True)\n",
            "loss 0.750349760055542\n",
            "gradient w: tensor([[-0.3990,  0.6706,  0.1255],\n",
            "        [-0.6236,  0.1537,  0.1243]])\n",
            "gradient b: tensor([ 0.5347, -0.2214])\n",
            "loss after one iteration 0.736577570438385\n",
            "gradient w: tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "gradient b: tensor([0., 0.])\n",
            "loss after one iteration 0.736577570438385\n",
            "gradient w: tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "gradient b: tensor([0., 0.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-EQo7OUtZS7",
        "outputId": "1b39c948-cb6b-4d88-df0c-b977ce691221"
      },
      "source": [
        "# define dataset and model here\r\n",
        "import torch.nn as nn\r\n",
        "# create torch tensor\r\n",
        "x=torch.randn(10,3)\r\n",
        "y=torch.randn(10,2)\r\n",
        "# build linear model\r\n",
        "linearm=nn.Linear(3,2)\r\n",
        "# build loss function and optimizers\r\n",
        "loss_fun=nn.MSELoss()\r\n",
        "optimizer=torch.optim.SGD(linearm.parameters(),lr=0.01)\r\n",
        "\r\n",
        "# compute loss and gradient for 10 epochs\r\n",
        "for epoch in range(10):\r\n",
        "  # forward \r\n",
        "  pred=linearm(x)\r\n",
        "  optimizer.zero_grad() # zero the gradient after each epoch\r\n",
        "  # compute loss\r\n",
        "  loss=loss_fun(pred,y)\r\n",
        "  # print the loss\r\n",
        "  print(f\"loss for {epoch}\",loss.item())\r\n",
        "  # backward\r\n",
        "  loss.backward()\r\n",
        "  #optimizer.zero_grad()\r\n",
        "  # print the gradients\r\n",
        "  print(f\"gradient w:{epoch}:\",linearm.weight.grad)\r\n",
        "  print(f\"gradient b:{epoch}\",linearm.bias.grad)\r\n",
        "  # update the gradient\r\n",
        "  optimizer.step()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss for 0 2.306525468826294\n",
            "gradient w:0: tensor([[ 0.1109,  0.0764, -0.1833],\n",
            "        [ 0.7244, -0.7563, -1.0000]])\n",
            "gradient b:0 tensor([-0.9023,  0.0085])\n",
            "loss for 1 2.2770919799804688\n",
            "gradient w:1: tensor([[ 0.1109,  0.0730, -0.1821],\n",
            "        [ 0.7183, -0.7448, -0.9826]])\n",
            "gradient b:1 tensor([-0.8935,  0.0035])\n",
            "loss for 2 2.248426914215088\n",
            "gradient w:2: tensor([[ 0.1109,  0.0695, -0.1809],\n",
            "        [ 0.7123, -0.7335, -0.9655]])\n",
            "gradient b:2 tensor([-0.8848, -0.0013])\n",
            "loss for 3 2.2205071449279785\n",
            "gradient w:3: tensor([[ 0.1109,  0.0662, -0.1797],\n",
            "        [ 0.7063, -0.7223, -0.9487]])\n",
            "gradient b:3 tensor([-0.8762, -0.0060])\n",
            "loss for 4 2.1933109760284424\n",
            "gradient w:4: tensor([[ 0.1109,  0.0629, -0.1784],\n",
            "        [ 0.7004, -0.7113, -0.9322]])\n",
            "gradient b:4 tensor([-0.8678, -0.0106])\n",
            "loss for 5 2.166815996170044\n",
            "gradient w:5: tensor([[ 0.1109,  0.0597, -0.1772],\n",
            "        [ 0.6946, -0.7005, -0.9159]])\n",
            "gradient b:5 tensor([-0.8593, -0.0151])\n",
            "loss for 6 2.1410017013549805\n",
            "gradient w:6: tensor([[ 0.1109,  0.0566, -0.1760],\n",
            "        [ 0.6889, -0.6899, -0.9000]])\n",
            "gradient b:6 tensor([-0.8510, -0.0195])\n",
            "loss for 7 2.1158485412597656\n",
            "gradient w:7: tensor([[ 0.1108,  0.0535, -0.1747],\n",
            "        [ 0.6833, -0.6795, -0.8843]])\n",
            "gradient b:7 tensor([-0.8428, -0.0238])\n",
            "loss for 8 2.0913360118865967\n",
            "gradient w:8: tensor([[ 0.1108,  0.0505, -0.1735],\n",
            "        [ 0.6777, -0.6692, -0.8688]])\n",
            "gradient b:8 tensor([-0.8347, -0.0279])\n",
            "loss for 9 2.067446231842041\n",
            "gradient w:9: tensor([[ 0.1108,  0.0476, -0.1723],\n",
            "        [ 0.6722, -0.6591, -0.8537]])\n",
            "gradient b:9 tensor([-0.8266, -0.0320])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xhvb3DuVweEJ",
        "outputId": "a7bcaa18-999f-4b47-bafe-ab871f3978fe"
      },
      "source": [
        "# define dataset and model here\r\n",
        "import torch.nn as nn\r\n",
        "# create torch tensor\r\n",
        "x=torch.randn(10,3)\r\n",
        "y=torch.randn(10,2)\r\n",
        "# build linear model\r\n",
        "linearm=nn.Linear(3,2)\r\n",
        "# build loss function and optimizers\r\n",
        "loss_fun=nn.MSELoss()\r\n",
        "optimizer=torch.optim.SGD(linearm.parameters(),lr=0.01)\r\n",
        "\r\n",
        "# compute loss and gradient for 10 epochs\r\n",
        "for epoch in range(10):\r\n",
        "  # forward path \r\n",
        "  pred=linearm(x)\r\n",
        "  #optimizer.zero_grad() if I did not put gradient zeros here\r\n",
        "  # compute loss\r\n",
        "  loss=loss_fun(pred,y)\r\n",
        "  # print the loss\r\n",
        "  print(f\"loss for {epoch}\",loss.item())\r\n",
        "  # backward path and compute the gradient\r\n",
        "  loss.backward()\r\n",
        "  #optimizer.zero_grad()\r\n",
        "  # print the gradients\r\n",
        "  print(f\"gradient w:{epoch}:\",linearm.weight.grad) # gradient adding after every epochs\r\n",
        "  print(f\"gradient b:{epoch}\",linearm.bias.grad)\r\n",
        "  # update the gradient, weights and biases \r\n",
        "  optimizer.step()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss for 0 0.72789466381073\n",
            "gradient w:0: tensor([[-0.1942, -0.1736,  0.0471],\n",
            "        [ 0.6225,  0.0927, -0.2185]])\n",
            "gradient b:0 tensor([ 0.0836, -0.2209])\n",
            "loss for 1 0.7222256660461426\n",
            "gradient w:1: tensor([[-0.3856, -0.3464,  0.0946],\n",
            "        [ 1.2376,  0.1849, -0.4370]])\n",
            "gradient b:1 tensor([ 0.1674, -0.4405])\n",
            "loss for 2 0.7111088037490845\n",
            "gradient w:2: tensor([[-0.5717, -0.5175,  0.1428],\n",
            "        [ 1.8380,  0.2759, -0.6556]])\n",
            "gradient b:2 tensor([ 0.2514, -0.6574])\n",
            "loss for 3 0.6949760913848877\n",
            "gradient w:3: tensor([[-0.7500, -0.6861,  0.1919],\n",
            "        [ 2.4167,  0.3653, -0.8744]])\n",
            "gradient b:3 tensor([ 0.3358, -0.8702])\n",
            "loss for 4 0.6744505167007446\n",
            "gradient w:4: tensor([[-0.9179, -0.8515,  0.2422],\n",
            "        [ 2.9668,  0.4525, -1.0931]])\n",
            "gradient b:4 tensor([ 0.4206, -1.0775])\n",
            "loss for 5 0.6503175497055054\n",
            "gradient w:5: tensor([[-1.0731, -1.0130,  0.2940],\n",
            "        [ 3.4819,  0.5371, -1.3117]])\n",
            "gradient b:5 tensor([ 0.5058, -1.2780])\n",
            "loss for 6 0.623489260673523\n",
            "gradient w:6: tensor([[-1.2136, -1.1698,  0.3473],\n",
            "        [ 3.9561,  0.6188, -1.5299]])\n",
            "gradient b:6 tensor([ 0.5914, -1.4700])\n",
            "loss for 7 0.5949636697769165\n",
            "gradient w:7: tensor([[-1.3376, -1.3214,  0.4022],\n",
            "        [ 4.3840,  0.6972, -1.7473]])\n",
            "gradient b:7 tensor([ 0.6772, -1.6521])\n",
            "loss for 8 0.5657804012298584\n",
            "gradient w:8: tensor([[-1.4434, -1.4673,  0.4587],\n",
            "        [ 4.7609,  0.7722, -1.9633]])\n",
            "gradient b:8 tensor([ 0.7630, -1.8227])\n",
            "loss for 9 0.5369759798049927\n",
            "gradient w:9: tensor([[-1.5296, -1.6068,  0.5165],\n",
            "        [ 5.0828,  0.8437, -2.1773]])\n",
            "gradient b:9 tensor([ 0.8485, -1.9801])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDYnQhh4m2JN",
        "outputId": "9756b745-0bf4-40c1-c0c2-a5e95f624fec"
      },
      "source": [
        "# check the gradient accumulation\r\n",
        "import torch\r\n",
        "weight=torch.ones(4,requires_grad=True)\r\n",
        "for epoch in range(3):\r\n",
        "  weightmodel=(weight*3).sum()\r\n",
        "  weightmodel.backward()\r\n",
        "  print(weight.grad) # accumluate the gradients at every epoch\r\n",
        "  #weight.grad.zero_()\r\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([6., 6., 6., 6.])\n",
            "tensor([9., 9., 9., 9.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hbwn4s5szHor",
        "outputId": "c9e879b5-1be6-4020-ed8e-139ab3d0903b"
      },
      "source": [
        "# check the gradient accumulation\r\n",
        "import torch\r\n",
        "weight=torch.ones(4,requires_grad=True)\r\n",
        "for epoch in range(3):\r\n",
        "  weightmodel=(weight*3).sum()\r\n",
        "  weightmodel.backward()\r\n",
        "  print(weight.grad)\r\n",
        "  weight.grad.zero_() # use grad_zero to get same gradient after every epoch"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HJzSfNQzdXo",
        "outputId": "dcad8814-a4f7-4257-ba39-c4ff497cd627"
      },
      "source": [
        "# check the gradient accumulation\r\n",
        "import torch\r\n",
        "weight=torch.ones(4,requires_grad=True)\r\n",
        "for epoch in range(3):\r\n",
        "  weightmodel=(weight*3).sum()\r\n",
        "  print(weight.grad)\r\n",
        "  weightmodel.backward()\r\n",
        "  print(weight.grad)\r\n",
        "  #print(weight.grad.zero_()) # use grad_zero to get same gradient after every epoch"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([6., 6., 6., 6.])\n",
            "tensor([6., 6., 6., 6.])\n",
            "tensor([9., 9., 9., 9.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDI4wpE3yAaC",
        "outputId": "35feef64-9da7-4ec3-ffd4-c56b3701c152"
      },
      "source": [
        "# very basic and simple exmaple using graident computation based on simple model\r\n",
        "import torch\r\n",
        "X=torch.tensor(1.)\r\n",
        "Y=torch.tensor(2.)\r\n",
        "w=torch.tensor(1., requires_grad=True)\r\n",
        "# forward pass\r\n",
        "y1=w*x\r\n",
        "# compute the loss\r\n",
        "loss=(y1-y)**2\r\n",
        "#backward pass\r\n",
        "loss.backward()\r\n",
        "print(loss)\r\n",
        "print(loss.item())\r\n",
        "# check the gradient\r\n",
        "print(w.grad)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1., grad_fn=<PowBackward0>)\n",
            "1.0\n",
            "tensor(-2.)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}